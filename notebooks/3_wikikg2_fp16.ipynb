{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FP16 embeddings on OGBL-WikiKG2\n",
    "\n",
    "<em>Copyright (c) 2023 Graphcore Ltd. All rights reserved.</em>\n",
    "\n",
    "The aim of this notebook is to show how to use FP16 weights to reduce memory requirements for KGE models and speed computations up. This is especially important when dealing with KGs with a large number of entities, whose embeddings - if stored in FP32 - would require too many IPUs.\n",
    "\n",
    "As a study case we look at the [ogbl-wikikg2](https://ogb.stanford.edu/docs/linkprop/#ogbl-wikikg2) dataset, a KG containing 2.5M entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+ssh://git@github.com/graphcore-research/bess-kge.git@review\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import poptorch\n",
    "import torch\n",
    "\n",
    "from besskge.batch_sampler import RandomShardedBatchSampler, RigidShardedBatchSampler\n",
    "from besskge.bess import EmbeddingMovingBessKGE, ScoreMovingBessKGE, TopKQueryBessKGE\n",
    "from besskge.dataset import KGDataset\n",
    "from besskge.embedding import NormalInitializer\n",
    "from besskge.loss import SampledSoftmaxCrossEntropyLoss\n",
    "from besskge.metric import Evaluation\n",
    "from besskge.negative_sampler import (\n",
    "    PlaceholderNegativeSampler,\n",
    "    RandomShardedNegativeSampler,\n",
    "    TripleBasedShardedNegativeSampler,\n",
    ")\n",
    "from besskge.scoring import TransE\n",
    "from besskge.sharding import PartitionedTripleSet, Sharding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sharding entities and triples\n",
    "\n",
    "The OGBL-WikiKG2 dataset can be downloaded and preprocessed with the built-in method of `KGDataset`. Sharding of entities and triples is performed as already seen in the previous notebook [biogk_training_inference](1_biokg_training_inference.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikikg = KGDataset.build_wikikg2(root=pathlib.Path(\"../datasets/wikikg2\"))\n",
    "\n",
    "print(f\"Number of entities: {wikikg.n_entity:,}\\n\")\n",
    "print(f\"Number of relation types: {wikikg.n_relation_type}\\n\")\n",
    "print(f\"Number of triples: \\n training: {wikikg.triples['train'].shape[0]:,} \\n validation/test: {wikikg.triples['valid'].shape[0]:,}\\n\")\n",
    "print(f\"Number of negative heads/tails for validation/test triples: {wikikg.neg_heads['valid'].shape[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on 4 IPUs\n",
    "n_shard = 4\n",
    "\n",
    "seed = 1234\n",
    "\n",
    "sharding = Sharding.create(n_entity=wikikg.n_entity, n_shard=n_shard, seed=seed)\n",
    "\n",
    "print(f\"Number of total entities: {sharding.n_entity:,}\\n\")\n",
    "\n",
    "print(f\"Number of shards: {sharding.n_shard}\\n\")\n",
    "\n",
    "print(f\"Number of entities in each shard: {sharding.max_entity_per_shard:,}\\n\")\n",
    "\n",
    "print(f\"Entity sharding:\\n {sharding.shard_and_idx_to_entity}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_triples = PartitionedTripleSet.create_from_dataset(dataset=wikikg, part=\"train\", sharding=sharding, partition_mode=\"ht_shardpair\")\n",
    "\n",
    "print(f\"Number of triples per (h,t) shardpair:\\n {train_triples.triple_counts}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interleaved training and validation (vs all)\n",
    "\n",
    "We use the `RandomShardedBatchSampler` class which creates a batch by randomly sampling (with replacement) a fixed number (=`batch_sampler.positive_per_partition`) of triples from each of the 16 shardpairs.\n",
    "\n",
    "Negative entities, used to construct negative samples, are also sampled randomly using the `RandomShardedNegativeSampler` class. Here we construct negative samples by always corrupting the tail of positive triples, as specified by the choice `corruption_scheme=\"t\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_iterations = 100\n",
    "accum_factor = 1\n",
    "shard_bs = 512\n",
    "\n",
    "neg_sampler = RandomShardedNegativeSampler(n_negative=32, sharding=sharding, seed=seed, corruption_scheme=\"t\",\n",
    "                                           local_sampling=False, flat_negative_format=True)\n",
    "\n",
    "batch_sampler = RandomShardedBatchSampler(partitioned_triple_set=train_triples, negative_sampler=neg_sampler,\n",
    "                              shard_bs=shard_bs, batches_per_step=device_iterations*accum_factor, seed=seed)\n",
    "\n",
    "\n",
    "print(f\"# triples per shardpair per step: {batch_sampler.positive_per_partition} \\n\")\n",
    "\n",
    "# Example batch\n",
    "idx_sampler = iter(batch_sampler.get_dataloader_sampler(shuffle=True))\n",
    "for k,v in batch_sampler[next(idx_sampler)].items():\n",
    "    print(f\"{k:<12} {str(v.shape):<30} {v.dtype};\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice from the `negative` shape that we are decoupling the number of negative samples from the batch size, by using the negative sampler option `flat_negative_format=True`. Instead of sampling negative entities on a triple basis, we sample them on a shardpair basis: each pair of devices will exchange $32$ negative entities in both directions (while, if using `flat_negative_format=False`, this number would need to be a multiple of the shard batch size - see for instance the shape of `negative` in the [biogk_training_inference](1_biokg_training_inference.ipynb) notebook). This of course requires the use of negative sample sharing.\n",
    "\n",
    "Thanks to this decoupling we can increase the shard batch size without increasing the number of negative samples to score in each batch. This is important as we don't intend to use gradient accumulation (as specified by the choice `accum_factor = 1`), thus saving the memory costs coming from the weight accumulation tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = poptorch.Options()\n",
    "options.replication_factor = sharding.n_shard\n",
    "options.deviceIterations(device_iterations)\n",
    "options._popart.setPatterns(dict(RemoveAllReducePattern=True))\n",
    "\n",
    "# Enable stochastic rounding on IPU for more stable half-precision training\n",
    "options.Precision.enableStochasticRounding(True)\n",
    "\n",
    "train_dl = batch_sampler.get_dataloader(options=options, shuffle=True, num_workers=3, persistent_workers=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train a simple **TransE** KGE model with an embedding size of $100$, in order to fit the embedding tables in the SRAM of 4 IPUs, and use sampled softmax cross entropy loss.\n",
    "\n",
    "For this dataset we find it to be beneficial to construct negative samples by corrupting the tail of a positive triple using not just the randomly sampled negative entities in `negative`, but also the tails of the other positive triples in the same microbatch. This can be done just by setting the flag `augment_negative=True` when instantiating the `EmbeddingMovingBessKGE` distribution scheme. This strategy also has the advantage of increasing the number of negative samples used in the contrastive loss without increasing the costs of gathering and communicating negative entities across IPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = SampledSoftmaxCrossEntropyLoss(n_entity=wikikg.n_entity)\n",
    "emb_initializer = NormalInitializer()\n",
    "transe_score_fn = TransE(negative_sample_sharing=True, scoring_norm=1, sharding=sharding,\n",
    "                  n_relation_type=wikikg.n_relation_type, embedding_size=100,\n",
    "                  entity_initializer=emb_initializer, relation_initializer=emb_initializer)\n",
    "\n",
    "model = EmbeddingMovingBessKGE(negative_sampler=neg_sampler, score_fn=transe_score_fn,\n",
    "                               loss_fn=loss_fn, augment_negative=True)\n",
    "\n",
    "print(f\"# model parameters: {model.n_embedding_parameters:,}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in standard PyTorch, we can just use `model.half()` to cast the embedding tables to FP16, before wrapping the model in the `poptorch.trainingModel`.\n",
    "\n",
    "We train with SGD momentum instead of Adam, for reduced optimizer state memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FP16 weights\n",
    "model.half()\n",
    "\n",
    "opt = poptorch.optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr=0.001,\n",
    "        momentum=0.95,\n",
    "        velocity_accum_type=torch.float16,\n",
    "    )\n",
    "\n",
    "poptorch_model = poptorch.trainingModel(model, options=options, optimizer=opt)\n",
    "\n",
    "# The variable entity_embedding needs to hold different values on each replica,\n",
    "# corresponding to the shards of the entity embedding table\n",
    "poptorch_model.entity_embedding.replicaGrouping(\n",
    "            poptorch.CommGroupType.NoGrouping,\n",
    "            0,\n",
    "            poptorch.VariableRetrievalMode.OnePerGroup,\n",
    "        )\n",
    "\n",
    "# Compile model\n",
    "batch = next(iter(train_dl))\n",
    "res = poptorch_model(**{k: v.flatten(end_dim=1) for k, v in batch.items()})\n",
    "\n",
    "poptorch_model.detachFromDevice()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all we need to do to use FP16 weights! The triple scoring in TransE via L1 distance is also gonna be performed in FP16, while - for stability reasons - the loss is always computed in FP32.\n",
    "\n",
    "To track the performance evolution during training, we will validate at regular cadence on a set of 4000 triples sampled randomly from the validation set. We provide **no tail candidates** (hence, scoring each (h,r,?) query against all 2.5 milion entities in the KG). As this would be too slow to do on CPU, we will perform the task on IPU using the `TopKQueryBessKGE` BESS module (see the [yago_topk_prediction](2_yago_topk_prediction.ipynb) notebook for more details on this class and how to use it).\n",
    "\n",
    "To partition a custom set of triples (that is, not one of the \"official\" triple parts specified when creating the `KGDataset`) on the fly, use the `PartitionedTripleSet.create_from_queries` function as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample_queries = 4000\n",
    "\n",
    "# Partition a random subset of n_sample_queries triples taken from wikikg.triples[\"valid\"]\n",
    "subset_val_triples = wikikg.triples[\"valid\"][np.random.RandomState(seed=seed).choice(wikikg.triples[\"valid\"].shape[0], n_sample_queries)]\n",
    "sample_val_triples = PartitionedTripleSet.create_from_queries(wikikg, sharding, queries=subset_val_triples[:,:2],\n",
    "                                                               query_mode=\"hr\", ground_truth=subset_val_triples[:,2])\n",
    "\n",
    "val_device_iterations = 2\n",
    "val_shard_bs = 512                                                             \n",
    "\n",
    "candidate_sampler = PlaceholderNegativeSampler(corruption_scheme=\"t\", seed=seed)\n",
    "bs_sample = RigidShardedBatchSampler(partitioned_triple_set=sample_val_triples, negative_sampler=candidate_sampler, shard_bs=val_shard_bs,\n",
    "                                    batches_per_step=val_device_iterations, seed=seed, duplicate_batch=False, return_triple_idx=False)\n",
    "\n",
    "print(\"Number of triples per h_shard:\")\n",
    "print(sample_val_triples.triple_counts)\n",
    "\n",
    "val_options = poptorch.Options()\n",
    "val_options.replication_factor = sharding.n_shard\n",
    "val_options.deviceIterations(bs_sample.batches_per_step)\n",
    "val_options.outputMode(poptorch.OutputMode.All)\n",
    "\n",
    "sample_valid_dl = bs_sample.get_dataloader(options=val_options, shuffle=False, num_workers=2, persistent_workers=True)\n",
    "\n",
    "# Example batch\n",
    "val_batch = next(iter(sample_valid_dl))\n",
    "for k,v in val_batch.items():\n",
    "    print(f\"{k:<12} {str(v.shape):<30}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference model\n",
    "\n",
    "evaluation = Evaluation([\"mrr\"], worst_rank_infty=True, reduction=\"sum\")\n",
    "\n",
    "inf_model = TopKQueryBessKGE(k=10, candidate_sampler=candidate_sampler, score_fn=transe_score_fn, evaluation=evaluation, window_size=500)\n",
    "\n",
    "poptorch_inf_model = poptorch.inferenceModel(inf_model, options=val_options)\n",
    "\n",
    "poptorch_inf_model.entity_embedding.replicaGrouping(\n",
    "            poptorch.CommGroupType.NoGrouping,\n",
    "            0,\n",
    "            poptorch.VariableRetrievalMode.OnePerGroup,\n",
    "        )\n",
    "\n",
    "# Compile inference model\n",
    "val_res = poptorch_inf_model(**{k: v.flatten(end_dim=1) for k, v in val_batch.items()})\n",
    "\n",
    "poptorch_inf_model.detachFromDevice()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us train the model for 70 epochs, performing validation on the selected random set of triples every 10 epochs. This should take less than 10 minutes.\n",
    "\n",
    "Be aware that the interleaved execution scheme introduces some overhead for detaching and attaching the executables of the two different models to the available IPUs. This overhead is *not* included in the time measurements that we print out, as it can be avoided by using two separates IPU-POD4, one for training and one for validation (if you are running a session with at least 8 IPUs, just remove the `model.detachFromDevice()`, `model.attachToDevice()` parts of the code in the above cells and in the next one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 70\n",
    "val_ep_interval = 10\n",
    "\n",
    "cumulative_triples = 0\n",
    "training_loss = []\n",
    "val_mrr = []\n",
    "poptorch_model.attachToDevice()\n",
    "for ep in range(n_epochs):\n",
    "    ep_start_time = time.time()\n",
    "    ep_log = []\n",
    "    for batch in train_dl:\n",
    "        step_start_time = time.time()\n",
    "        cumulative_triples += batch[\"head\"].numel()\n",
    "        res = poptorch_model(**{k: v.flatten(end_dim=1) for k, v in batch.items()})\n",
    "        ep_log.append(dict(loss= float(torch.sum(res[\"loss\"])) / batch[\"head\"][0].numel(), step_time=(time.time()-step_start_time)))\n",
    "    ep_loss = [v['loss'] for v in ep_log]\n",
    "    training_loss.extend([v['loss'] for v in ep_log])\n",
    "    print(f\"Epoch {ep+1} loss: {np.mean(ep_loss):.6f} --- positive triples processed: {cumulative_triples:.2e}\")\n",
    "    print(f\"Epoch duration (sec): {(time.time() - ep_start_time):.5f} (average step time: {np.mean([v['step_time'] for v in ep_log]):.5f})\")\n",
    "    if ep % val_ep_interval == 0:\n",
    "        poptorch_model.detachFromDevice()\n",
    "        poptorch_inf_model.attachToDevice()\n",
    "        val_start_time = time.time()\n",
    "        ep_mrr = 0.0\n",
    "        for batch_val in sample_valid_dl:\n",
    "            ep_mrr += poptorch_inf_model(**{k: v.flatten(end_dim=1) for k, v in batch_val.items()})[\"metrics\"].sum()\n",
    "        ep_mrr /= n_sample_queries  \n",
    "        val_mrr.append(ep_mrr)\n",
    "        print(f\"Epoch {ep+1} sample MRR: {ep_mrr:.4f} (validation time: {(time.time() - val_start_time):.5f})\")\n",
    "        poptorch_inf_model.detachFromDevice()\n",
    "        poptorch_model.attachToDevice()\n",
    "\n",
    "# Plot loss and sample MRR as a function of the number of positive triples processed\n",
    "total_triples = np.cumsum(n_epochs * len(train_dl) * [batch[\"head\"].numel()])\n",
    "ax0, ax1 = plt.gca(), plt.twinx()\n",
    "line0, = ax0.plot(total_triples, training_loss)\n",
    "line1, = ax1.plot(total_triples[::val_ep_interval * len(train_dl)], val_mrr, color=\"r\")\n",
    "ax0.set_xlabel(\"Positive triples\")\n",
    "ax0.set_xlim(xmin=5e5)\n",
    "ax0.set_ylabel(\"Loss\")\n",
    "ax1.set_ylabel(\"Sample MRR\")\n",
    "plt.legend([line0, line1], [\"Loss\", \"MRR\"], loc=\"upper left\")\n",
    "\n",
    "poptorch_model.detachFromDevice()\n",
    "del train_dl\n",
    "del sample_valid_dl"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us compute the final MRR on the whole validation set, containing 429k+ triples. Once again, even though the dataset provides 500 candidate tails for each triple, here we are actually scoring queries against all entities in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_triples = PartitionedTripleSet.create_from_dataset(wikikg, \"valid\", sharding, partition_mode=\"h_shard\")\n",
    "bs_valid = RigidShardedBatchSampler(partitioned_triple_set=validation_triples, negative_sampler=candidate_sampler, shard_bs=val_shard_bs,\n",
    "                                    batches_per_step=val_device_iterations, seed=seed)\n",
    "\n",
    "print(\"Number of triples per h_shard:\")\n",
    "print(validation_triples.triple_counts)\n",
    "\n",
    "valid_dl = bs_valid.get_dataloader(options=val_options, shuffle=False, num_workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poptorch_inf_model.attachToDevice()\n",
    "\n",
    "val_mrr = 0.0\n",
    "start_time = time.time()\n",
    "n_val_queries = 0\n",
    "for batch_val in valid_dl:\n",
    "    n_val_queries += batch_val[\"triple_mask\"].sum()\n",
    "    val_mrr += poptorch_inf_model(**{k: v.flatten(end_dim=1) for k, v in batch_val.items()})[\"metrics\"].sum()\n",
    "\n",
    "print(f\"Validation MRR: {val_mrr / n_val_queries}\")\n",
    "print(f\"Validation time (sec): {(time.time() - start_time):.5f}\")\n",
    "\n",
    "poptorch_inf_model.detachFromDevice()\n",
    "del valid_dl"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation (vs candidate tails)\n",
    "\n",
    "To conclude, let's check the metrics scoring validation queries only against the provided candidates.\n",
    "\n",
    "Similarly to what we did in the [biogk_training_inference](1_biokg_training_inference.ipynb) notebook, we use the `TripleBasedShardedNegativeSampler` to provide the model with the triple-specific candidate tails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_triples = PartitionedTripleSet.create_from_dataset(dataset=wikikg, part=\"valid\", sharding=sharding, partition_mode=\"ht_shardpair\")\n",
    "ns_valid = TripleBasedShardedNegativeSampler(negative_heads=validation_triples.neg_heads, negative_tails=validation_triples.neg_tails,\n",
    "                                             sharding=sharding, corruption_scheme=\"t\", seed=seed)\n",
    "# We do not need to duplicate_batch as we only want to score negative tails\n",
    "bs_valid = RigidShardedBatchSampler(partitioned_triple_set=validation_triples, negative_sampler=ns_valid, shard_bs=256, batches_per_step=10,\n",
    "                                    seed=seed, duplicate_batch=False)\n",
    "\n",
    "# Example batch\n",
    "idx_sampler = iter(bs_valid.get_dataloader_sampler(shuffle=False))\n",
    "for k,v in bs_valid[next(idx_sampler)].items():\n",
    "    print(f\"{k:<15} {str(v.shape):<35} {v.dtype};\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see from the shape of `negative` and `negative_mask`, the 500 negative tails for each validation triple are retrieved in blocks of $172$ entities from each of the $4$ IPUs (with padding where needed).\n",
    "\n",
    "We use the `ScoreMovingBessKGE` flavour of BESS, as recommended for performing inference against triple-specific sets of candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_options = poptorch.Options()\n",
    "val_options.replication_factor = sharding.n_shard\n",
    "val_options.deviceIterations(bs_valid.batches_per_step)\n",
    "val_options.outputMode(poptorch.OutputMode.All)\n",
    "\n",
    "valid_dl = bs_valid.get_dataloader(options=val_options, shuffle=False, num_workers=3, persistent_workers=True)\n",
    "\n",
    "# Each triple is now to be scored against a specific set of negatives, so we turn off negative sample sharing\n",
    "transe_score_fn.negative_sample_sharing = False\n",
    "\n",
    "evaluation = Evaluation([\"mrr\", \"hits@1\", \"hits@5\", \"hits@10\"], reduction=\"sum\")\n",
    "model_inf = ScoreMovingBessKGE(negative_sampler=ns_valid, score_fn=transe_score_fn, evaluation=evaluation)\n",
    "\n",
    "poptorch_model_inf = poptorch.inferenceModel(model_inf, options=val_options)\n",
    "\n",
    "poptorch_model_inf.entity_embedding.replicaGrouping(\n",
    "            poptorch.CommGroupType.NoGrouping,\n",
    "            0,\n",
    "            poptorch.VariableRetrievalMode.OnePerGroup,\n",
    "        )\n",
    "\n",
    "# Compile model\n",
    "batch = next(iter(valid_dl))\n",
    "res = poptorch_model_inf(**{k: v.flatten(end_dim=1) for k, v in batch.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_log = []\n",
    "start_time = time.time()\n",
    "n_val_queries = 0\n",
    "for batch_val in valid_dl:\n",
    "    res = poptorch_model_inf(**{k: v.flatten(end_dim=1) for k, v in batch_val.items()})\n",
    "    n_val_queries += batch_val[\"triple_mask\"].sum()\n",
    "    # By transposing res[\"metrics\"] we separate the outputs for the different metrics\n",
    "    val_log.append({k: v.sum() for k, v in zip(\n",
    "                        evaluation.metrics.keys(),\n",
    "                        res[\"metrics\"].T,\n",
    "                    )})\n",
    "\n",
    "for metric in val_log[0].keys():\n",
    "    reduced_metric = sum([l[metric] for l in val_log]) / n_val_queries\n",
    "    print(\"%s : %f\" % (metric, reduced_metric))\n",
    "print(f\"Validation time (sec): {(time.time() - start_time):.5f}\")\n",
    "\n",
    "poptorch_model_inf.detachFromDevice()\n",
    "del valid_dl"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On this dataset, predicting tails is significantly easier than predicting heads. Want to see how the model we trained performs on the complete task (scoring queries against candidate heads + candidate tails), to compare with the TransE (100dim) result on the [OGB leaderboard](https://ogb.stanford.edu/docs/leader_linkprop/#ogbl-wikikg2)? Just set `corruption_scheme=\"ht\"` when creating the `TripleBasedShardedNegativeSampler` and `duplicate_batch=True` in the `RigidShardedBatchSampler` and re-run the last three code cells. If this is the aim, however, you might want to try corrupting both heads and tails during training: just change the corrpution scheme of the `RandomShardedNegativeSampler` at the beginning of [training](#interleaved-training-and-validation-vs-all)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv3.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
